---
layout: post
title: Performance Guaranteed Network Acceleration via High-Order Residual Quantization
---

Scholars from Shanghai Jiaotong Universities has proposed a new method for compressing the neural networks. Their method is able to reduce the size of the network to `1/30` while maintaining the accuracies.

![Architecture](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0RzlHzChHia6uZV92FMlcibEUI1Ycfn6gAWteAwwvA3FkNBLSYIZKA0mTuORcibRPFaEtwIOPUVxtmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

Different from existing pruning method or quantization method, the proposed method is based on recursively binarizing the residuals of the network: in each stage, the residual of the compressed network is binarized to form a new bit; then the new compressed network is applied to compute the residual.

![Experiment result on MNIST](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0RzlHzChHia6uZV92FMlcibEtYoWCd7cAECdaibANPSrLibSwYk9WxqOx7fjQYAXJibroTSW7SNusGcRQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

![Experiment result on Cifar10](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0RzlHzChHia6uZV92FMlcibEZO76OXxgJM8icqyTULy1zXhj0MyvNJTEf9k6uZrWXoyGDSRPOr8AJpA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

![Speedup ratio over number of bits](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0RzlHzChHia6uZV92FMlcibESySqWRkgZf0aCDPTDWKb1cGibweaWZ4rFPibLjsvCVBTUOLHeRibTeg5Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)
