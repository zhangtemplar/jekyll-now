---
layout: post
title: Machine Learning Translation and the Google Translate Algorithm
---

[Daniil Korbut](https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup) has kinderly written an introduction to [Machine Learning Translation and the Google Translate Algorithm](https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4?nsukey=VImjuDSmJU5s7XG%20cVgu5QIzrc0KecpUzpUdvUXxifDycOG5DXilsiMxZ4Tvs9ja9XQJ6EKe0fBnYzJwuvbqQw3MaRxVOHkwmW9xkA75dDpNwJM4VEBiPYXLER34D0txiOrVl2pS3flSlaIh9pq72G9ZPolOBX6io%20dJtOk%2Fki51k1r5W82uKRxKNYWGHtjs). I have summarized its content here.

# The Problem

Years ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence.

![Google Machine Translation](https://cdn-images-1.medium.com/max/1600/0*AOe3ERun08zcfN1S.)

The old method relies on defining a lot of grammar rules manually. However there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down.

# Solution

```
Modern machine translation systems use a different approach: they allocate the rules from text by analyzing a huge set of documents.
```

## Recurrent Neural Networks
Here is where `Long Short-Term Memory networks` (LSTMs) come into play, helping us to work with sequences whose length we can’t know a priori. LSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules.

![Unrolled recurrent neural network](https://cdn-images-1.medium.com/max/1600/0*cIuGYhTOWpMudvHy.)

## Bidirectional RNNs
Our next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states.

![Bidirectional recurrent neural networks](https://cdn-images-1.medium.com/max/1600/1*bACdupA0mGvIO2ijlUOTbA.png)

## Sequence to sequence
Now we’re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output.

![Sequence to sequence model](https://cdn-images-1.medium.com/max/1600/0*Jf7KcjSTpRu7ifFL.)

# Metric

Researchers are using [BLEU](https://en.wikipedia.org/wiki/BLEU) (bilingual evaluation understudy) to evaluate the performance of the machine translation: 

# Other readings

  - [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - [A Neural Network for Machine Translation, at Production Scale](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html)
  - [Google’s Neural Machine Translation System](https://arxiv.org/abs/1609.08144)
