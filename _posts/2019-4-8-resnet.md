---
layout: post
title: ResNet V1 vs V2
tags: resnet deep-learning shortcut pre-activation post-activation activation
---

Since proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf), ResNet has drawn a lot of interests, especially its capabability of training a very deep nerual network (from 19 of VGG19 to 50 or even 200 layers). Later ResNet V2 was proposed in [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf).

The difference between the V1 and V2 is (as illustrated below):
- V1: Convolution then batch normalization then ReLU
- V2: Batch normalization then ReLu then convolution

The motivation behind is that V2 is much easier to train and generalizes better than the V1 ResNet

![](https://cdn-images-1.medium.com/max/1200/1*V2FgD6udOE4xJuu_R7L6qA.png)

In fact, other variations has been explored as well and obviously not all variations work:

![](https://raw.githubusercontent.com/aleju/papers/master/neural-nets/images/Identity_Mappings_in_Deep_Residual_Networks__shortcuts.png?raw=true)

![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f/8-Table2-1.png)
